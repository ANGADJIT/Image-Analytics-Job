# User Profile Image Emotion Extraction Data Batch Job

## Overview
![job](https://github.com/ANGADJIT/Image-Analytics-Job/assets/67195682/fe361794-4f6f-4631-b6a4-4b5d181b3cc4)
This project involves a batch job to perform analytics on user profile images to extract emotions, age, gender, and race attributes. The batch job is divided into two Airflow DAGs:

1. **User Entry DAG**: This DAG fetches random user-generated data from the [Random User API](https://randomuser.me/api/), processes and models this data, and stores it in a database. The information stored includes:
   - username
   - address
   - email
   - profile_url

2. **Emotion Extraction DAG**: This DAG retrieves the profile images of the users generated by the first DAG, analyzes the emotions using the Deepface library, and stores the results in a new table called `useremotions`. Metadata is stored in Airflow variables to avoid reprocessing users that have already been processed. The information stored includes:
   - emotion
   - age
   - gender
   - race

## Tech Stack Used
- **Airflow** for batch jobs
- **Postgres** for databases
- **Python** as the language used by Airflow



## How to Set Up the Project
![Screenshot from 2024-07-07 22-06-57](https://github.com/ANGADJIT/Image-Analytics-Job/assets/67195682/92187377-20d4-47ff-ba16-be1673429b99)
1. **Clone the Project from GitHub**
   ```bash
   git clone <repository-url>
   cd <repository-directory>
   ```

2. **Create a Python Virtual Environment**
   ```bash
   python3.10 -m venv venv
   source venv/bin/activate
   ```

3. **Export Environment Variables**
   ```bash
   export PYTHONPATH=${PYTHONPATH}:<path-to-project>/Image-Analytics-Job/dags
   export AIRFLOW_CONFIG=<path-to-project>/Image-Analytics-Job/config/<config-file-name>.cfg
   ```

4. **Initialize the Airflow Database**
   ```bash
   airflow db init
   ```

5. **Run Airflow**
   ```bash
   airflow standalone
   ```
   This will generate a file named `standalone_admin_password.txt` where `admin` will be the username. Use the password from this file.

6. **Set Up Connection for Custom Postgres Hook**
   - Go to Connections
   - Connection type: HTTP
   - Click on Add
   - Enter the following details:
     - Conn ID: connection id
     - Schema: database name
     - Host
     - Login: username
     - Password

7. **Configure Airflow Variables**
   Go to `localhost:8000 > Admin > Variables` and set the following:
   - `USER_DB_CONN_ID`: Custom Postgres hook connection ID
   - `USER_TABLE_NAME`: User table name
   - `RESULTS`: Number of users to fetch from the API (e.g., 10)
   - `NAT`: Nationality code (e.g., IN for India)
   - `USER_EMOTIONS_TABLE_NAME`: User emotion data table name

## How to Test Locally
1. Go to any DAG and add `dag.test()` at the root level of the script.
2. Hit F5 to run the test.
3. Note: Change paths in `launch.json` according to your project structure.
4. Remove `dag.test()` when Airflow is running.
